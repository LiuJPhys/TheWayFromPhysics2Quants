\contentsline {section}{\numberline {1}Policy Gradient Methods for Reinforcement Learning with Function Approximation}{2}{section.1}% 
\contentsline {section}{\numberline {2}Eligibility Traces}{4}{section.2}% 
\contentsline {subsection}{\numberline {2.1}On policy}{4}{subsection.2.1}% 
\contentsline {subsection}{\numberline {2.2}Off policy}{7}{subsection.2.2}% 
\contentsline {section}{\numberline {3}GTD--gradient temporal difference}{7}{section.3}% 
\contentsline {subsection}{\numberline {3.1} Policy gradient methods for reinforcement learning with function approximation}{7}{subsection.3.1}% 
\contentsline {subsection}{\numberline {3.2}Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation}{8}{subsection.3.2}% 
\contentsline {subsection}{\numberline {3.3}GTD$(\lambda )$}{9}{subsection.3.3}% 
\contentsline {section}{\numberline {4}Off Policy Actor-Critic}{12}{section.4}% 
\contentsline {section}{\numberline {5}DDPG}{15}{section.5}% 
\contentsline {subsection}{\numberline {5.1}Determinisitic Policy Gradient Algorithms}{15}{subsection.5.1}% 
\contentsline {section}{\numberline {6}TRPO}{18}{section.6}% 
\contentsline {section}{\numberline {7}PPO}{20}{section.7}% 
\contentsline {subsection}{\numberline {7.1}clipped Surrogate Objective}{21}{subsection.7.1}% 
\contentsline {section}{\numberline {8}A3C}{21}{section.8}% 
\contentsline {section}{\numberline {A}Lists of Proofs}{23}{appendix.A}% 
\contentsline {subsection}{\numberline {A.1}Eligibility Traces}{23}{subsection.A.1}% 
\contentsline {subsubsection}{\numberline {A.1.1}Equivalence between forward and backward}{23}{subsubsection.A.1.1}% 
\contentsline {subsection}{\numberline {A.2}TRPO}{24}{subsection.A.2}% 
\contentsline {subsubsection}{\numberline {A.2.1}Proof Th.\ref {th:TRPO_lowerbound}}{24}{subsubsection.A.2.1}% 
