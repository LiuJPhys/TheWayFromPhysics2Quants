\contentsline {section}{\numberline {1}Policy Gradient Methods for Reinforcement Learning with Function Approximation}{2}{section.1}% 
\contentsline {section}{\numberline {2}Eligibility Traces}{4}{section.2}% 
\contentsline {subsection}{\numberline {2.1}On policy}{4}{subsection.2.1}% 
\contentsline {subsection}{\numberline {2.2}Off policy}{5}{subsection.2.2}% 
\contentsline {section}{\numberline {3}GTD--gradient temporal difference}{5}{section.3}% 
\contentsline {subsection}{\numberline {3.1} Policy gradient methods for reinforcement learning with function approximation}{5}{subsection.3.1}% 
\contentsline {subsection}{\numberline {3.2}Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation}{6}{subsection.3.2}% 
\contentsline {subsection}{\numberline {3.3}GTD$(\lambda )$}{7}{subsection.3.3}% 
\contentsline {section}{\numberline {4}Off Policy Actor-Critic}{10}{section.4}% 
\contentsline {section}{\numberline {5}DDPG}{13}{section.5}% 
\contentsline {subsection}{\numberline {5.1}Determinisitic Policy Gradient Algorithms}{13}{subsection.5.1}% 
\contentsline {section}{\numberline {6}TRPO}{16}{section.6}% 
\contentsline {section}{\numberline {7}PPO}{18}{section.7}% 
\contentsline {subsection}{\numberline {7.1}clipped Surrogate Objective}{19}{subsection.7.1}% 
\contentsline {section}{\numberline {8}A3C}{19}{section.8}% 
\contentsline {section}{\numberline {A}Lists of Proofs}{21}{appendix.A}% 
\contentsline {subsection}{\numberline {A.1}Eligibility Traces}{21}{subsection.A.1}% 
\contentsline {subsubsection}{\numberline {A.1.1}Equivalence between forward and backward}{21}{subsubsection.A.1.1}% 
\contentsline {subsection}{\numberline {A.2}TRPO}{21}{subsection.A.2}% 
\contentsline {subsubsection}{\numberline {A.2.1}Proof Th.\ref {th:TRPO_lowerbound}}{21}{subsubsection.A.2.1}% 
