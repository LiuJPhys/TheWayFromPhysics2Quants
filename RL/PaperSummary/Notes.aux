\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sutton2000policy}
\@writefile{toc}{\contentsline {section}{\numberline {1}Policy Gradient Methods for Reinforcement Learning with Function Approximation}{2}{section.1}\protected@file@percent }
\newlabel{eq:approx_q}{{1.19}{3}{Policy Gradient Methods for Reinforcement Learning with Function Approximation}{equation.1.19}{}}
\newlabel{eq: cons}{{1.20}{3}{Policy Gradient with Function Approximation}{equation.1.20}{}}
\citation{sutton1998introduction}
\@writefile{toc}{\contentsline {section}{\numberline {2}Eligibility Traces}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}On policy}{4}{subsection.2.1}\protected@file@percent }
\citation{seijen2014true}
\newlabel{eq: trace}{{2.17}{6}{On policy}{equation.2.17}{}}
\citation{precup2000eligibility}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Off policy}{7}{subsection.2.2}\protected@file@percent }
\newlabel{alg: online_et_op}{{2.2}{7}{Off policy}{equation.2.22}{}}
\citation{NIPS2008_3626}
\newlabel{th:et_th1}{{2.2.1}{8}{}{theorem.2.2.1}{}}
\newlabel{alg: online_et_treebackup}{{2.2}{8}{Off policy}{theorem.2.2.1}{}}
\newlabel{th:et_th2}{{2.2.2}{8}{}{theorem.2.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}GTD--gradient temporal difference}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Policy gradient methods for reinforcement learning with function approximation}{8}{subsection.3.1}\protected@file@percent }
\citation{sutton2009fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Fast Gradient-Descent Methods for Temporal-Difference Learning with Linear Function Approximation}{9}{subsection.3.2}\protected@file@percent }
\citation{maei2011gradient}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}GTD$(\lambda )$}{10}{subsection.3.3}\protected@file@percent }
\citation{degris2012off}
\@writefile{toc}{\contentsline {section}{\numberline {4}Off Policy Actor-Critic}{13}{section.4}\protected@file@percent }
\newlabel{eq:tabular1}{{4.15}{15}{Off Policy Actor-Critic}{equation.4.15}{}}
\citation{silver2014deterministic}
\@writefile{toc}{\contentsline {section}{\numberline {5}DDPG}{16}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Determinisitic Policy Gradient Algorithms}{16}{subsection.5.1}\protected@file@percent }
\citation{schulman2015trust}
\@writefile{toc}{\contentsline {section}{\numberline {6}TRPO}{19}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The black curse is our true objective, and we optimize red lower bound in local blue region.}}{20}{figure.1}\protected@file@percent }
\newlabel{eq:TRPO_eta}{{6.2}{20}{TRPO}{equation.6.2}{}}
\newlabel{eq: TRPO_eta1}{{6.4}{20}{TRPO}{equation.6.4}{}}
\citation{schulman2017proximal}
\newlabel{th:TRPO_lowerbound}{{6.0.1}{21}{}{theorem.6.0.1}{}}
\newlabel{eq:TRPO_problem}{{6.10}{21}{TRPO}{equation.6.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}PPO}{21}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}clipped Surrogate Objective}{22}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}A3C}{22}{section.8}\protected@file@percent }
\citation{wang2016sample}
\@writefile{toc}{\contentsline {section}{\numberline {9}ACER}{23}{section.9}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{RL}
\bibcite{sutton2000policy}{{1}{}{{}}{{}}}
\bibcite{sutton1998introduction}{{2}{}{{}}{{}}}
\bibcite{seijen2014true}{{3}{}{{}}{{}}}
\bibcite{precup2000eligibility}{{4}{}{{}}{{}}}
\bibcite{NIPS2008_3626}{{5}{}{{}}{{}}}
\bibcite{sutton2009fast}{{6}{}{{}}{{}}}
\bibcite{maei2011gradient}{{7}{}{{}}{{}}}
\bibcite{degris2012off}{{8}{}{{}}{{}}}
\bibcite{silver2014deterministic}{{9}{}{{}}{{}}}
\bibcite{schulman2015trust}{{10}{}{{}}{{}}}
\bibcite{schulman2017proximal}{{11}{}{{}}{{}}}
\bibcite{wang2016sample}{{12}{}{{}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Lists of Proofs}{26}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Eligibility Traces}{26}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Equivalence between forward and backward}{26}{subsubsection.A.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Proofs of Theorem \ref  {th:et_th1}}{27}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.2.1}Proof of Theorem \ref  {th:et_th2}}{28}{subsubsection.A.2.1}\protected@file@percent }
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}TRPO}{29}{subsection.A.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.3.1}Proof Th.\ref  {th:TRPO_lowerbound}}{29}{subsubsection.A.3.1}\protected@file@percent }
\newlabel{lem: TRPO1}{{A.3.1}{29}{}{lemma.A.3.1}{}}
