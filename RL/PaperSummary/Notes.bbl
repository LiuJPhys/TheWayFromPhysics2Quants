\begin{thebibliography}{1}

\bibitem{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem{NIPS2008_3626}
Richard~S Sutton, Hamid~R. Maei, and Csaba Szepesv\'{a}ri.
\newblock A convergent o(n) temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In D.~Koller, D.~Schuurmans, Y.~Bengio, and L.~Bottou, editors, {\em
  Advances in Neural Information Processing Systems 21}, pages 1609--1616.
  Curran Associates, Inc., 2009.

\bibitem{sutton2009fast}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 993--1000. ACM, 2009.

\bibitem{maei2011gradient}
Hamid~Reza Maei.
\newblock Gradient temporal-difference learning algorithms.
\newblock 2011.

\bibitem{degris2012off}
Thomas Degris, Martha White, and Richard~S Sutton.
\newblock Off-policy actor-critic.
\newblock {\em arXiv preprint arXiv:1205.4839}, 2012.

\bibitem{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em ICML}, 2014.

\bibitem{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\end{thebibliography}
